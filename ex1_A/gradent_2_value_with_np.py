# -*- coding: utf-8 -*-
"""gradient-l-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19FLZW-zKnQsiIym2RZ9-FF8cs_DhYiCM

Code taken from: https://www.kaggle.com/rakend/simple-linear-regression-using-gradient-descent with small changes
"""

import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt

# Load the data set
from sklearn.datasets import load_boston
boston_data = load_boston()
#print(boston_data['DESCR'])

"""If you print(boston_data['DESCR']) you get a description of the fields below.  RM is the number of rooms in the house and is the field we will focus on right now."""

boston_data['feature_names']
df = pd.DataFrame(data=boston_data['data'])

df.columns = boston_data['feature_names']
df

"""Price is the cost in thousands of dollars."""

df['Price'] =  boston_data['target']
df

"""This is the reason we focus on RM. It is the highest positively correlated feature.  LSTAT actually is more correlated but negatively. (LSTAT is the social status of the people living in the houses.)"""

corr = df.corr()
corr['Price'].sort_values(ascending=False)

"""This code does two things. First, it standardizes the values.  Also, it plots them after the standardization. """

x = df[['RM']]
y = df['Price']
x = (x - x.mean())/x.std()
#y = (y - y.mean())/y.std()
plt.scatter(x,y)
plt.xlabel('Number of rooms per house', size = 20)
plt.ylabel('House Price', size = 20)
plt.show()
#x

x = np.c_[np.ones(x.shape[0]),x] #adds ones for the weights
# Parameters required for Gradient Descent
alpha = 0.05   #learning rate
m = y.size  #no. of samples
np.random.seed(10)
theta = np.random.rand(2)  #initializing theta with some random values for slope
#x

"""Now the fun starts in batch.  Note that we are using MSE -- 1/(2*m) but the error is simply prediction - y.  Also, as we do things in Numpy the batch for the whole matrix is simply np.dot(error.T, error)."""

def gradient_descent(x, y, m, theta,  alpha):
    cost_list = []   #to record all cost values to this list
    theta_list = []  #to record all theta_0 and theta_1 values to this list
    prediction_list = []
    run = True
    cost_list.append(1e10)    #we append some large value to the cost list
    i=0
    while run:
        prediction = np.dot(x, theta)   #predicted y values theta_1*x1+theta_2*x2
        prediction_list.append(prediction)
        error = prediction - y #compare to y_actual[i] - y_predicted[i] in other file
        cost = 1/(2*m) * np.dot(error.T, error)   #  (1/2m)*sum[(error)^2]
        cost_list.append(cost)
        theta = theta - (alpha * (1/m) * np.dot(x.T, error))
        # compare to theta = theta - learning_rate*grad in other file
        theta_list.append(theta)
        if cost_list[i]-cost_list[i+1] < 1e-7:   #checking if the change in cost function is less than 10^(-9)
            run = False

        i+=1
    cost_list.pop(0)   # Remove the large number we added in the begining
    return prediction_list, cost_list, theta_list

"""This might take a second, but now output the results!"""

prediction_list, cost_list, theta_list = gradient_descent(x, y, m, theta, alpha)
theta = theta_list[-1]
print(theta)

"""Visualizations of the slope convergence"""

plt.title('Cost Function J', size = 30)
plt.xlabel('No. of iterations', size=20)
plt.ylabel('Cost', size=20)
plt.plot(cost_list)
plt.show()

def y_predicted(theta,x):
    y_pred = np.zeros(len(x))
    for i in range(0,len(x)-1):
        for j in range(0,len(theta)-1):
            y_pred[i] = y_pred[i]+(theta[j]*x[i][j])#for the weights
        y_pred[i]+= theta[-1] #for the bias
    return y_pred

def regression_test(x_test,theta):
    row = x_test.shape[0]
    column = x_test.shape[1]
    new_x_test = np.ones((row,column+1))
    new_x_test[:,0:column] = x_test
    y_pred = y_predicted(theta,new_x_test)
    return(y_pred)

pred_df = pd.DataFrame(
    {
        'Actual Value' : y,
     'Predicted Values' :  np.dot(x,theta),
    }
)
print(pred_df.head(10))