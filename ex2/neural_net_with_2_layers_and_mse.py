# -*- coding: utf-8 -*-
"""Neural net with 2 layers and MSE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vo0PekXT0NuBIP0STYUKWUZ9KCJMTUkI

Adapted from the homeworks with the simple logistic model from the Deep Learning Toar Sheni class last year with Dr. Elishai Ezra Tzur. We will now add one hidden layer with 3 nodes and see how that changes things.
"""



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt

"""### sigmoid(z)

Here we define our activation function; the sigmoid function 

s = $g(\theta^{T}x)$

$z = \theta^{T}x$

$g(z) = \frac{1}{1+e^{(-z)}}$

X := data set

$\theta$ := vector of weights

Compute the sigmoid of z (A scalar or numpy array of any size) returns s

Verify: sigmoid([0, 2]) = [ 0.5, 0.88079708]
"""

def sigmoid(z):
  X = np.exp(z)
  return X/(1+X)

def sigmoid_der(z):
  A = sigmoid(z)
  return A*(1-A)

def relu(z):
    return np.maximum(0,z)

relu_der = lambda x : np.array([(i > 0) * 1 for i in x])

# def relu_der(z):
#
#     for n,i in enumerate(z):
#         for n1,j in enumerate(i):
#             z[n][n1]=0 if j<0 else 1
#     return z
    # return np.array([0 if i<0 else 1 for i in [y for y in z]])
"""Some examples of using this function. Notice that we can give it an array of values (not critical for us)"""

print(sigmoid([0,2]))
print(sigmoid(2))
print(sigmoid(np.array([4])))

"""### initialize_with_random: w, b
We don't use inialize with zero as zero values can be bad as we discussed in class.
Instead we inialize with random numbers.
"""

""" Initialize w and b for the both layers according to the number of the features and number of neurons in the layers.
W should be initialized randomly to small values (otherwise, values at the activation functions could be at the flat part).
"""
def initialize_parameters (n_x, n_h, n_y):
    return {
    "W1":np.random.randn(n_h,n_x) * 0.01,
    "b1":np.zeros([n_h, 1]),
    "W2":np.random.randn(n_y,n_h) * 0.01,
    "b2":np.zeros([n_y, 1]),
}

# Toy example
print(initialize_parameters(4,3,1))

"""### forward propagate(X, thetas): 
retuns: A2 (the final value) and the cache of values
Implement the forward propagation
* parameters -- python dictionary containing your parameters (output of initialization function)
Note that thetas is now a cache of thetas (weights) 
* A2 -- The sigmoid output of the second activation
* cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
"""

def forward_propagation(X, parameters,func):
    #Hidden Layer
    Z1 = parameters["W1"].dot(X)+parameters["b1"]
    A1 = func(Z1) #change here for the targil
    #Output Layer
    Z2 = parameters["W2"].dot(A1)+parameters["b2"]
    A2 = sigmoid(Z2)
    #print(A2)
    cache = {
        "Z1":Z1,
        "A1":A1,
        "Z2":Z2,
        "A2":A2
    }
    return A2, cache

"""Back_propagation calcuates the weight updates using the derivative of the different activation functions. These equations are similar to those in the lecture."""

def tanh_der(z): 
    X=np.tanh(z)
    return 1-X**2

def backward_propagation(parameters, cache, X, Y,func):
    m = X.shape[1] # Number of samples
    #Assuming the Log_loss function is used -- like last time:
    #Output Layer--similar to the last time
    #dZ2 = cache["A2"] - Y #for the sigmoid layer
    #dW2 = (1 / m) * dZ2.dot(cache["A1"].T)
    #db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    #Output Layer
    dA2 =  -1 * (Y- cache["A2"]) #The derivative of MSE is -(Y-YP) (derivative of cost)
    dZ2 = dA2 * sigmoid_der(cache["Z2"]) #output derivative * node derivative
    dW2 = (1 / m) * np.dot(dZ2,cache["A1"].T ) #for the input- A1 is the input to the second level, as X is the input to the first level
    db2 = (1 / m) * np.sum(dZ2)
    #db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    #Hidden Layer
    dA1 = np.dot(parameters["W2"].T, dA2)
    dZ1 =  dA1 * func(cache["Z1"])  #change here for the targil
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1)
    #db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    return {"dW1":dW1,"dW2":dW2,"db1":db1,"db2":db2}

"""The cost function from the last example."""

def MSE_calculation(A,Y):
    m=A.shape[1]
    E=Y-A
    cost = np.sum(E**2)
    return cost/(2*m)
'''
def LogLoss_calculation(A,Y):
    cost = np.mean(-(Y*np.log(A) + (1-Y)*np.log(1-(A))))  
    return cost'''

"""Update the weights in the dictionary cache."""

def update_parameters(parameters, grads, learning_rate):
    return {
    "W1": parameters["W1"] - learning_rate*grads["dW1"],
    "W2": parameters["W2"] - learning_rate*grads["dW2"],
    "b1": parameters["b1"] - learning_rate*grads["db1"],
    "b2": parameters["b2"] - learning_rate*grads["db2"],
}

"""### nn_model(X, Y, num_iterations, learning_rate): d
Builds the logistic regression model by calling the functions implemented above
* X_train -- training set represented by a numpy array of shape (number of features, m_train)
* Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
* X_test -- test set represented by a numpy array of shape (number of features, m_test)
* Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
* num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
* learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
* d -- dictionary containing information about the model. 
"""

def nn_model(X, Y, iterations,lr,nh,func,func_der):
    n_x=X.shape[0]
    n_h=nh  #change here for the targil
    n_y=1
    parameters = initialize_parameters(n_x,n_h,n_y)
    print("Network shape " , X.shape[0], n_h , n_y)
    for i in range(iterations):
        A2, cache = forward_propagation(X,parameters,func)
        cost = MSE_calculation(A2,Y)
        #cost = LogLoss_calculation(A2,Y)
        grads = backward_propagation(parameters,cache,X,Y,func_der)
        parameters = update_parameters(parameters,grads,lr)
        costs.append(cost)
        #cost check
        if i % 100 == 0:
            print (f"Cost after iteration {i}: {cost}")
    return parameters, costs

"""### predict(X, parameters): Y_prediction"""

def predict(X, parameters,func):
    A2, cache = forward_propagation(X, parameters,func)
    return np.rint(A2)
    '''This round the values like:
    def predict(X, theta, threshold=0.5):
    if predict_probs(X, theta) >= threshold:
        return 1
        print(A)'''

def prediction_accuracy(y_pred,y_true):
    return np.mean(y_pred==y_true)

import pandas as pd
from sklearn.model_selection import train_test_split

'''
here we are
'''

# creat 3 tabel for the testing:
# relu
testing_relu_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
testing_relu_df = pd.DataFrame(testing_relu_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])


# tanh
testing_tanh_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
testing_tanh_df = pd.DataFrame(testing_tanh_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])

# sigmoid
testing_sigmoid_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
testing_sigmoid_df = pd.DataFrame(testing_sigmoid_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])

# creat 3 tabel for the training:


# tanh
training_tanh_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
training_tanh_df = pd.DataFrame(training_tanh_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])
# relu
training_relu_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
training_relu_df = pd.DataFrame(training_relu_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])
# sigmoid
training_sigmoid_data= {
    'alpha':[0]*6,
    '500 iter':[0]*6,
    '1000 iter':[0]*6,
    '1500 iter':[0]*6,
    '2000 iter':[0]*6

}
training_sigmoid_df = pd.DataFrame(training_sigmoid_data, index=['1 nodes','2 nodes','3 nodes','4 nodes','5 nodes','6 nodes'])


url = 'https://github.com/rosenfa/nn/blob/master/pima-indians-diabetes.csv?raw=true'
#url = 'https://github.com/rosenfa/nn/blob/master/class2/spam.csv?raw=true'
df=pd.read_csv(url,  header=0, error_bad_lines=False)
features = df.drop(['Outcome'], axis = 1 )
features = ((features - features.mean())/features.std())
X = np.array(features)
Y = np.array(df['Outcome'])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)
df

from sklearn.linear_model import LogisticRegression
sk_model = LogisticRegression()
sk_model.fit(X_train, Y_train)
accuracy = sk_model.score(X_test, Y_test)
print("accuracy = ", accuracy * 100, "%")
#print(Y_train)

X_train, X_test = X_train.T, X_test.T

num_iterations=2000 #number of iterations  #change here for the targil
alpha = 1 #learning rate
costs = []
# parameters, costs = nn_model(X_train, Y_train,num_iterations,alpha)
# Y_train_predict = predict(X_train, parameters)
# train_acc = prediction_accuracy(Y_train_predict,Y_train)
# Y_test_predict = predict(X_test, parameters)
# test_acc = prediction_accuracy(Y_test_predict,Y_test)
# parameters["train_accuracy"] = train_acc
# parameters["test_accuracy"] = test_acc

# plt.plot(costs)

# print("Training acc : ", str(train_acc))
# print("Testing acc : ", str(test_acc))

#funcs = [relu]
funcs = [np.tanh,sigmoid,relu]
#funcs_der=[relu_der]
funcs_der=[tanh_der,sigmoid_der,relu_der]
df_train=[training_tanh_df,training_sigmoid_df,training_relu_df]
df_test=[testing_tanh_df,testing_sigmoid_df,testing_relu_df]
for i in range(len(funcs)):
    for iterations in range(500,2001,500):
        for nh in range(1,7):
            parameters, costs = nn_model(X_train,Y_train,iterations,alpha,nh,funcs[i],funcs_der[i])
            Y_train_predict = predict(X_train, parameters,funcs[i])
            train_acc = prediction_accuracy(Y_train_predict, Y_train)
            Y_test_predict = predict(X_test, parameters,funcs[i])
            test_acc = prediction_accuracy(Y_test_predict, Y_test)
            df_train[i][f'{iterations} iter'][f'{nh} nodes'] = train_acc
            df_test[i][f'{iterations} iter'][f'{nh} nodes'] = test_acc
    print(df_train[i])
    print(df_test[i])

    plot_train=df_train[i].plot(title= f'training {funcs[i]}')
    ax1 = plt.gca()
    ax1.set_ylim([0.7, 0.9])
    plot_test=df_test[i].plot(title= f'testing {funcs[i]}')
    ax = plt.gca()
    ax.set_ylim([0.7, 0.9])
    plot_train.get_figure().savefig(f'output_train{i}.pdf', format='pdf')
    plot_test.get_figure().savefig(f'output_test{i}.pdf', format='pdf')
ax=plt.gca()
ax.set_ylim([0.7,0.9])
plt.show()


